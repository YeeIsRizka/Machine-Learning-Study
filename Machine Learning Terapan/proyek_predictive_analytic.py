# -*- coding: utf-8 -*-
"""proyek-predictive-analytic.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1k_REuIR9ScIs7DE0MFH71NycEOg3rsJt

# Proyek Predictive Analytics: [Student Habits vs Academic Performance Dataset]
- **Nama:** Rizka Alfadilla
- **Email:** rizkaal874@gmail.com
- **ID Dicoding:** rizka_alfadilla

## Import Semua Packages/Library yang Digunakan
"""

# Library yang sering digunakan
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import  OneHotEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from scipy.stats import uniform, randint

"""## Data Loading

Dataset = https://www.kaggle.com/datasets/jayaantanaath/student-habits-vs-academic-performance/data
"""

# Load hour.csv dari folder data dan tampilkan
df = pd.read_csv('student_habits_performance.csv')
df

"""### Exploratory Data Analysis (EDA)"""

df.info()

df.describe()

df.isnull().sum()

# Membuat dataframe untuk kondisi dataset
dataset_info = pd.DataFrame({
    "Tipe Data": df.dtypes,
    "Jumlah Missing": df.isnull().sum(),
    "Jumlah Unik": df.nunique(),
    "Nilai Unik Contoh": [df[col].unique()[:5].tolist() for col in df.columns]
})

dataset_info

# Melihat jumlah missing values sebelum imputasi
print("Jumlah missing values sebelum imputasi:")
print(df['parental_education_level'].isnull().sum())

# Imputasi missing values dengan 'Unknown'
# Mengatasi warning dengan assignment langsung
df['parental_education_level'] = df['parental_education_level'].fillna('Unknown')

# Melihat jumlah missing values setelah imputasi
print("\nJumlah missing values setelah imputasi:")
print(df['parental_education_level'].isnull().sum())

# Mengecek distribusi setelah imputasi
print("\nDistribusi 'parental_education_level' setelah imputasi:")
print(df['parental_education_level'].value_counts())

print("Jumlah duplikasi: ", df.duplicated().sum())

# Fitur numerik
numerical_features = ['age', 'study_hours_per_day', 'social_media_hours',
                      'netflix_hours', 'attendance_percentage', 'sleep_hours',
                      'exercise_frequency', 'mental_health_rating', 'exam_score']
# Fitur kategorikal
categorical_features = ['gender', 'part_time_job', 'diet_quality',
                        'parental_education_level', 'internet_quality',
                        'extracurricular_participation']

# Loop untuk setiap fitur kategorikal
for feature in categorical_features:
    # Hitung jumlah sampel dan persentase
    count = df[feature].value_counts()
    percent = 100 * df[feature].value_counts(normalize=True)

    # Membuat dataframe untuk visualisasi
    df_cat = pd.DataFrame({'jumlah sampel': count, 'persentase': percent.round(1)})
    print(f"\nDistribusi untuk fitur: {feature}\n")
    print(df_cat)
    print("-" * 50)

    # Visualisasi bar plot
    plt.figure(figsize=(8, 6))
    ax = count.plot(kind='bar', color='skyblue', edgecolor='black')
    plt.title(f"Distribusi {feature}")
    plt.xlabel(feature)
    plt.ylabel("Jumlah Sampel")
    plt.xticks(rotation=45)
    plt.grid(axis='y', linestyle='--', alpha=0.7)

    # Menambahkan jumlah sampel di atas bar
    for i, val in enumerate(count):
        plt.text(i, val + 1, str(val), ha='center', va='bottom', fontsize=10, fontweight='bold', color='black')

    plt.tight_layout()
    plt.show()

# Visualisasi histogram untuk semua fitur numerik
df[numerical_features].hist(bins=50, figsize=(20, 15), color='skyblue', edgecolor='black')
plt.suptitle("Distribusi Fitur Numerik", fontsize=16, y=1.02)
plt.tight_layout()
plt.show()

# Visualisasi rata-rata `exam_score` berdasarkan fitur kategorikal
for col in categorical_features:
    plt.figure(figsize=(12, 6))

    # Menghitung rata-rata `exam_score`
    data = df.groupby(col)['exam_score'].mean().reset_index()

    # Plot barplot
    ax = sns.barplot(x=col, y="exam_score", data=data, palette="Set3", ci=None)
    plt.title(f"Rata-rata 'exam_score' terhadap {col}")
    plt.xticks(rotation=45)
    plt.ylabel("Rata-rata Exam Score")
    plt.xlabel(col)

    # Menampilkan rata-rata `exam_score` di atas bar
    for i, bar in enumerate(ax.patches):
        height = bar.get_height()
        mean_score = round(height, 2)  # Membulatkan rata-rata exam_score
        ax.text(
            bar.get_x() + bar.get_width() / 2,  # Posisi x (tengah bar)
            height + 0.5,  # Posisi y (sedikit di atas bar)
            f'{mean_score}',  # Teks rata-rata exam_score
            ha='center', va='bottom', fontsize=10, fontweight='bold', color='black'
        )

    plt.tight_layout()
    plt.show()

# Mengamati hubungan antar fitur numerik dengan fungsi pairplot()
sns.pairplot(df, diag_kind = 'kde')

# Membuat heatmap untuk korelasi fitur numerik
plt.figure(figsize=(10, 8))
correlation_matrix = df[numerical_features].corr().round(2)

# Plot heatmap
sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title("Correlation Matrix untuk Fitur Numerik", fontsize=20)
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

"""## Data Preparation

### Data Preprocessing
"""

df.drop(['age'], inplace=True, axis=1)
df.drop(['student_id'], inplace=True, axis=1)

df.info()

label_features = ['diet_quality', 'parental_education_level', 'internet_quality']

# Inisialisasi Label Encoder
le = LabelEncoder()

# Melakukan Label Encoding
for feature in label_features:
    df[feature] = le.fit_transform(df[feature])

print("\nData setelah Label Encoding:")
df[label_features].head()

# Inisialisasi OneHotEncoder
encoder = OneHotEncoder(drop='first', sparse_output=False)  # drop='first' untuk menghindari dummy variable trap

# Fitur nominal yang akan di-One Hot Encoding
one_hot_features = ['gender', 'part_time_job', 'extracurricular_participation']

# Melakukan One Hot Encoding
encoded_data = encoder.fit_transform(df[one_hot_features])

# Mendapatkan nama kolom hasil encoding
encoded_columns = encoder.get_feature_names_out(one_hot_features)

# Membuat DataFrame dari hasil encoding
encoded_df = pd.DataFrame(encoded_data, columns=encoded_columns)

# Menggabungkan hasil encoding ke DataFrame asli
df = pd.concat([df.drop(one_hot_features, axis=1), encoded_df], axis=1)

# Menampilkan hasil One Hot Encoding
print("\nData setelah One Hot Encoding:")
df.head()

numerical_features = ['study_hours_per_day', 'social_media_hours',
                      'netflix_hours', 'attendance_percentage', 'sleep_hours',
                      'exercise_frequency', 'mental_health_rating']

# Inisialisasi StandardScaler
scaler = StandardScaler()

# Melakukan standarisasi (hanya untuk fitur numerik)
df[numerical_features] = scaler.fit_transform(df[numerical_features])

# Menampilkan hasil standarisasi
print("\nData setelah Standarisasi:")
df.head()

"""### Split Dataset"""

X = df.drop('exam_score', axis=1)
y = df['exam_score']

# Membagi dataset menjadi training set (80%) dan testing set (20%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Menampilkan ukuran dataset
print(f"Ukuran X_train: {X_train.shape}")
print(f"Ukuran X_test: {X_test.shape}")
print(f"Ukuran y_train: {y_train.shape}")
print(f"Ukuran y_test: {y_test.shape}")

"""## Model Development"""

# === 1. Parameter distributions ===
param_distributions = {
    "Linear Regression": {
        "fit_intercept": [True, False],
        "copy_X": [True, False],
        "positive": [True, False]
    },
     "Random Forest": {
        "n_estimators": [50, 100, 150, 200],
        "max_depth": [3, 5, 7, 10],
        "min_samples_split": [2, 5, 10],
        "min_samples_leaf": [1, 2, 4]
    },
    "Gradient Boosting": {
        "n_estimators": [50, 100, 200],
        "max_depth": [3, 5, 7],
        "learning_rate": [0.01, 0.05, 0.1, 0.2],
        "subsample": [0.5, 0.7, 1.0]
    }
}

# === 2. Model list ===
model_list = {
    "Linear Regression": LinearRegression(),
    "Random Forest": RandomForestRegressor(random_state=42),
    "Gradient Boosting": GradientBoostingRegressor(random_state=42)
}

# === 3. Tuning, Training, Evaluation ===
results = []

for model_name, model in model_list.items():
    print(f"\nTuning {model_name}...")

    search = GridSearchCV(  # Use GridSearchCV instead for small parameter space
        estimator=model,
        param_grid=param_distributions[model_name],
        cv=3,
        scoring='neg_mean_squared_error',
        n_jobs=-1,
        verbose=1
    )

    # Melakukan pencarian hyperparameter terbaik
    search.fit(X_train, y_train)
    best_model = search.best_estimator_

    # Prediksi
    train_pred = best_model.predict(X_train)
    test_pred = best_model.predict(X_test)

    # Evaluasi
    train_mse = mean_squared_error(y_train, train_pred)
    test_mse = mean_squared_error(y_test, test_pred)
    train_mae = mean_absolute_error(y_train, train_pred)
    test_mae = mean_absolute_error(y_test, test_pred)
    train_r2 = r2_score(y_train, train_pred)
    test_r2 = r2_score(y_test, test_pred)

    # Menyimpan hasil evaluasi
    results.append({
        "Model": model_name,
        "Best Params": search.best_params_,
        "Train MSE": f"{train_mse:.2f}",
        "Test MSE": f"{test_mse:.2f}",
        "Train MAE": f"{train_mae:.2f}",
        "Test MAE": f"{test_mae:.2f}",
        "Train R2": f"{train_r2:.2f}",
        "Test R2": f"{test_r2:.2f}"
    })

    print(f"{model_name}: Train MSE = {train_mse:.2f}, Test MSE = {test_mse:.2f}, Train R2 = {train_r2:.2f}, Test R2 = {test_r2:.2f}")

# === 4. Simpan Results ===
df_results = pd.DataFrame(results)

"""## Evaluasi Model"""

print("\nHasil Evaluasi Model:")
df_results