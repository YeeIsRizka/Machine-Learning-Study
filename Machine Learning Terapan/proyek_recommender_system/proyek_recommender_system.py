# -*- coding: utf-8 -*-
"""proyek-recommender-system.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nFKE8jNWOdiEhrzb4bZBc2vBNbC9f-_Q

# Proyek Recommender System: [BoardGameGeek Reviews Dataset]
- **Nama:** Rizka Alfadilla
- **Email:** rizkaal874@gmail.com
- **ID Dicoding:** rizka_alfadilla

## Import Semua Packages/Library yang Digunakan

Pada bagian ini, dilakukan import seluruh library yang akan digunakan dalam proyek ini. Library yang digunakan meliputi Pandas, Matplotlib, Seaborn, dan library machine learning dari scikit-learn dan tensorflow.
"""

# Library yang sering digunakan
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Library machine learning
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from itertools import combinations
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# Hubungkan google drive dengan google colab
from google.colab import drive
drive.mount('/content/drive/')

"""## Data Understanding

Dataset = https://www.kaggle.com/datasets/jvanelteren/boardgamegeek-reviews/data

Dataset diunduh dari Kaggle (BoardGameGeek Reviews Dataset). Dataset ini berisi kumpulan ulasan (reviews) pengguna tentang board game yang dikumpulkan dari situs BoardGameGeek (BGG) — situs terbesar di dunia untuk penggemar board game.

### Data Loading

Memuat dataset dari google drive untuk digunakan.
"""

df_game = pd.read_csv('drive/MyDrive/data/rekomendasi/games_detailed_info.csv', index_col=0)
df_game.head()

df_review = pd.read_csv('drive/MyDrive/data/rekomendasi/bgg-15m-reviews.csv', index_col=0)
df_review.head()

"""### Exploratory Data Analysis (EDA)

Pada bagian ini, dilakukan EDA untuk memahami distribusi data dan missing values. Visualisasi menggunakan Seaborn dan Matplotlib.
"""

df_game.info()

"""Dataset ini memiliki 21.631 sampel dengan 55 fitur yang terdiri dari 21 fitur kategorikal (`object`), 18 fitur numerik kontinu (`float64`), dan 16 fitur numerik diskrit (`int64`). Beberapa fitur juga memiliki nilai kosong (missing values) dalam jumlah signifikan, misalnya fitur `alternate`, `boardgamemechanic`, dan `boardgameartist`. Dataset ini adalah metadata detail tentang board game, bukan review dari pengguna secara langsung, tapi lebih ke informasi deskriptif dari masing-masing permainan.

"""

df_review.info()

"""Dataset ini memiliki 15.823.269 sampel dengan 5 fitur, terdiri dari 3 fitur kategorikal (`object`) yaitu `user`, `comment`, dan `name`, 1 fitur numerik kontinu (`float64`) yaitu `rating`, serta 1 fitur numerik diskrit (`int64`) yaitu `ID`. Dataset ini berisi data ulasan pengguna terhadap berbagai board game, skala datanya juga sangat besar sehingga memakan banyak memori.

"""

df_game.describe()

"""Beberapa fitur menunjukkan distribusi yang luas dan kemungkinan adanya outlier, contohnya `playingtime`, `minplaytime`, dan `maxplaytime` yang memiliki nilai maksimum hingga 60.000 menit, jauh di atas nilai 75% yaitu 90 menit. Fitur `yearpublished` memiliki nilai minimum ekstrem -3500 yang tampaknya merupakan data error atau artefak historis. Sebagian besar ranking genre seperti `Thematic Rank`, `War Game Rank`, dan `Customizable Rank` hanya terisi sebagian kecil game, dengan fitur seperti `RPG Item Rank`, `Accessory Rank`, dan lainnya hanya memiliki satu nilai unik (tanpa variasi), yang membuatnya kurang informatif dan berpotensi dihapus."""

df_review.describe()

"""Fitur `rating` memiliki nilai rata-rata sebesar **7,05** dengan standar deviasi sekitar **1,6**, yang menunjukkan bahwa sebagian besar game mendapatkan penilaian yang cenderung positif dari pengguna. Nilai median berada di **7**, dengan kuartil bawah dan atas di **6** dan **8**, sehingga distribusinya simetris dan terpusat.

"""

def summarize_dataframe(df, sample_unique=5):
    summary = pd.DataFrame({
        "Tipe Data": df.dtypes,
        "Jumlah Data": df.count(),
        "Jumlah Missing": df.isnull().sum(),
        "Jumlah Unik": df.nunique(),
        "Contoh Nilai Unik": [df[col].dropna().unique()[:sample_unique].tolist() for col in df.columns]
    })

    return summary

summarize_dataframe(df_game)

"""Beberapa kolom seperti `boardgameexpansion`, `boardgameimplementation`, dan `boardgameintegration` memiliki proporsi missing value yang sangat besar (lebih dari 70%). Fitur `boardgamecategory`, `boardgamemechanic`, dan `boardgamefamily` memiliki jumlah data unik yg cukup banyak serta mengandung informasi tematik dan mekanisme gameplay, sehingga dapat digunakan sebagai fitur untuk membangun sistem content-based recommendation.



"""

summarize_dataframe(df_review)

"""Kolom `rating` lengkap tanpa missing dan memiliki nilai detail, cocok untuk collaborative filtering. Kolom `comment` memiliki missing value sangat tinggi (>80%), tapi bisa dimanfaatkan untuk analisis sentimen jika diperlukan. Data ini dapat digunakan untuk membangun sistem rekomendasi collaborative filtering."""

print("Jumlah duplikasi data game: ", df_game.duplicated().sum())
print("Jumlah duplikasi data review: ", df_review.duplicated().sum())

"""Dalam kedua dataset yang akan digunakan tidak ditemukan data yang double atau duplikat."""

sns.histplot(df_review['rating'], bins=50, kde=True)
plt.title('Distribusi Rating')
plt.xlabel('Rating')
plt.ylabel('Jumlah')
plt.show()

"""Distribusi rating ditampilkan untuk memahami pola penilaian pengguna secara keseluruhan, mengidentifikasi skewness data, serta mendeteksi outlier atau ketidakseimbangan pada nilai rating. Terlihat bahwa sistribusi rating  sangat terpusat pada bagian kanan. Ini menunjukkan adanya skewness ke arah rating positif, yang umum terjadi pada data ulasan pengguna."""

# Bagi rating menjadi interval
bins = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
labels = ['0–1', '1–2', '2–3', '3–4', '4–5', '5–6', '6–7', '7–8', '8–9', '9–10']
df_review['rating_bin'] = pd.cut(df_review['rating'], bins=bins, labels=labels, include_lowest=True, right=True)

# Hitung frekuensi
interval_counts = df_review['rating_bin'].value_counts().sort_index()

# Buat Diagramnya
plt.figure(figsize=(10, 6))
bars = plt.bar(interval_counts.index, interval_counts.values, color='skyblue', edgecolor='black')
for bar in bars:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2.0, height, f'{int(height)}', ha='center', va='bottom')
plt.title('Distribusi Rating Berdasarkan Interval')
plt.xlabel('Interval Rating')
plt.ylabel('Jumlah')
plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

"""Distribusi rating menunjukkan bahwa sebagian besar rating berada pada rentang 6–8, dengan puncak tertinggi pada interval 6–7 dan 7–8, masing-masing melebihi 4 juta entri. Hal ini mengindikasikan bahwa pengguna cenderung memberikan penilaian positif."""

# Hitung jumlah rating per game
rating_per_game = df_review['name'].value_counts().reset_index()
rating_per_game.columns = ['game_name', 'rating_count']

# Tampilkan 10 game teratas yang paling banyak diberi rating
print("10 Game dengan Jumlah Rating Terbanyak")
print(rating_per_game.head(10))
print()

top_games = rating_per_game.head(10)
plt.barh(top_games['game_name'][::-1], top_games['rating_count'][::-1])
plt.xlabel('Jumlah Rating')
plt.title('10 Game dengan Jumlah Rating Terbanyak')
plt.show()

"""Menampilkan 10 game dengan jumlah rating terbanyak untuk mengidentifikasi game-game paling populer berdasarkan keterlibatan pengguna (user engagement). Game seperti Pandemic, Carcassonne, dan Catan memiliki basis pengguna yang sangat besar dan aktif, sehingga perlu diperhatikan lebih apakah mungkin dapat mempengaruhi sistem rekomendasi secara signifikan atau tidak."""

rating_per_user = df_review['user'].value_counts()

print(rating_per_user)
print()

rating_per_user.hist(bins=100, figsize=(10, 6))
plt.xlabel('Jumlah Rating oleh User')
plt.ylabel('Jumlah User')
plt.title('Distribusi Rating per User')
plt.yscale('log')  # Biar terlihat distribusi long-tail
plt.grid(True)
plt.show()

"""Terdapat sebagian kecil pengguna (seperti leffe dubbel, TomVasel, dan Doel) yang memberikan ribuan rating, sementara mayoritas pengguna hanya memberikan satu rating. Hal ini mengonfirmasi adanya distribusi long-tail yang sangat ekstrem. Berdasarkan ini memunculkan kecurigaan adanya banyak data yang dapat dianggap sebagai outlier.

"""

# Hitung jumlah rating per user
rating_per_user = df_review['user'].value_counts().reset_index()
rating_per_user.columns = ['user', 'rating_count']

# Ekstrak nilai jumlah rating
data_rating = rating_per_user['rating_count']

# Hitung Q1 dan Q3
q25_rating, q75_rating = np.percentile(data_rating, 25), np.percentile(data_rating, 75)

# Hitung IQR
iqr_rating = q75_rating - q25_rating

# Tentukan batas bawah dan atas
cut_off = iqr_rating * 1.5
minimum_rating = q25_rating - cut_off
maximum_rating = q75_rating + cut_off

# Deteksi user outlier
outlier_users_df = rating_per_user[
    (rating_per_user["rating_count"] < minimum_rating) |
    (rating_per_user["rating_count"] > maximum_rating)
]

# Filter data review yang berasal dari user outlier
df_review_outlier = df_review[df_review['user'].isin(outlier_users_df['user'])]

print(f"Jumlah data outlier: {len(df_review_outlier)}")
print(f"Jumlah User Outlier: {len(outlier_users_df)}")
outlier_users_df.head()

"""Terdapat 38.214 user yang dianggap sebagai outlier yang menghasilkan sekitar 9,6 juta data review—jumlah yang signifikan dan dapat menyebabkan bias pada model jika tidak ditangani. Langkah selanjutnya adalah mengevaluasi apakah outlier ini akan dihapus, dipisahkan, atau diberi perlakuan khusus (misalnya diberi bobot) tergantung pada pendekatan sistem rekomendasi yang akan dibangun, agar model tetap adil dan tidak terlalu condong ke aktivitas pengguna ekstrem.

## Data Preparation

### Data Preprocessing

Pada tahap ini, dilakukan preprocessing data. Langkah-langkah yang dilakukan meliputi penghapusan data outlier, kolom yang tidak relevan,  missing value dan penggabungan dataset.
"""

# Buat DataFrame baru tanpa review dari user outlier
df_review_cleaned = df_review[~df_review['user'].isin(outlier_users_df['user'])]

# Cek hasilnya
print("Jumlah data awal:", len(df_review))
print(f"Jumlah data setelah outlier dihapus: {len(df_review_cleaned)}\n")

rating_per_user = df_review_cleaned['user'].value_counts()

print(rating_per_user)
print()

rating_per_user.hist(bins=100, figsize=(10, 6))
plt.xlabel('Jumlah Rating oleh User')
plt.ylabel('Jumlah User')
plt.title('Distribusi Rating per User')
plt.yscale('log')
plt.grid(True)
plt.show()

"""Penghapusan data outlier dilakukan untuk membersihkan data dari aktivitas pengguna yang terlalu ekstrem, baik yang memberikan terlalu banyak maupun terlalu sedikit rating, agar model rekomendasi tidak terdistorsi oleh bias interaksi yang tidak representatif. Setelah pembersihan, jumlah data berkurang drastis dari 15,8 juta menjadi 6,1 juta, yang menunjukkan bahwa sebagian besar interaksi berasal dari user outlier. Langkah ini bertujuan untuk meningkatkan kualitas data pelatihan dengan menjaga distribusi interaksi yang lebih seimbang dan realistis, serta mengurangi risiko overfitting terhadap preferensi segelintir pengguna super aktif."""

# Mengambil kolom yang dibutuhkan
df_review_filtered = df_review_cleaned[['user', 'rating', 'ID']].copy()
df_review_filtered.rename(columns={'ID': 'gameId', 'user': 'username'}, inplace=True)
df_review_filtered.reset_index(drop=True, inplace=True)

# Mengatasi missing value
print("Jumlah Missing:")
print(df_review_filtered.isnull().sum())
print()
df_review_filtered.dropna(inplace=True)

# Cek data
print(df_review_filtered.info(verbose=True, show_counts=True))
df_review_filtered.sample(5)

"""Pada tahap ini dipilih kolom yang relevan (user, game ID, dan rating), mengganti nama kolom agar lebih konsisten dan mudah digunakan dalam pipeline juga meringankan beban komputasi, serta menghapus data yang mengandung missing values untuk menghindari error atau bias dalam proses training."""

# Ambil kolom yang diperlukan
df_game_filtered = df_game[['id', 'primary', 'boardgamecategory']].copy()

# Ganti nama kolom
df_game_filtered.rename(columns={
    'id': 'gameId',
    'primary': 'gameName',
    'boardgamecategory': 'category'
}, inplace=True)

# Mengatasi missing value
print("Jumlah Missing:")
print(df_game_filtered.isnull().sum())
print()
df_game_filtered.dropna(inplace=True)

# Cek data
print(df_game_filtered.info())
df_game_filtered.sample(5)

"""Langkah ini dilakukan untuk menyiapkan metadata game yang akan digunakan dalam sistem rekomendasi berbasis konten (content-based filtering), dengan fokus pada nama game dan kategori sebagai fitur deskriptif. Data difilter untuk hanya menyisakan kolom penting (gameId, gameName, dan category), kemudian dilakukan pembersihan dari missing value agar fitur-fitur ini dapat diolah tanpa error pada tahap ekstraksi fitur (misalnya TF-IDF atau encoding)."""

df_merged = pd.merge(
    df_review_filtered,
    df_game_filtered,
    on='gameId',
    how='inner'
)

# Cek data hasil join
print(df_merged.info(verbose=True, show_counts=True))
df_merged.sample(5)

"""Langkah ini dilakukan untuk menggabungkan data ulasan pengguna dengan metadata game berdasarkan kolom gameId, sehingga setiap ulasan memiliki informasi lengkap mengenai game yang diulas. Penggabungan ini penting untuk membangun sistem rekomendasi, khususnya pada pendekatan collaborative filtering yang dapat dipadukan dengan konten game. Proses merge dengan metode inner join memastikan hanya data dengan gameId yang cocok di kedua tabel yang dipertahankan, sehingga hasilnya bersih dari data yang tidak lengkap atau tidak relevan. Terlihat terjadi pengurangan data dari 6149361  menjadi 6130738, menandakan sudah tidak ada data yg tidak lengkap dalam pembangunan kedua dataset ini/"""

data_review = df_review_filtered[
    df_review_filtered['username'].isin(df_merged['username']) &
    df_review_filtered['gameId'].isin(df_merged['gameId'])
].copy()

data_game = df_game_filtered[
    df_game_filtered['gameId'].isin(df_merged['gameId'])
].copy()

print("Data Review Final:")
print(data_review.info(verbose=True, show_counts=True))
print("\nData Game Final:")
print(data_game.info(verbose=True, show_counts=True))

"""Langkah ini dilakukan untuk menyelaraskan data review dan data game agar hanya mencakup entri yang telah berhasil digabungkan sebelumnya, sehingga menghindari ketidaksesuaian data pada tahap pemodelan. Dengan memfilter `data_review` dan `data_game` berdasarkan `gameId` dan `username` yang valid di `df_merged`, diperoleh dataset final yang konsisten dan siap digunakan untuk pelatihan model rekomendasi.

### Content Based Filltering Preparation

#### Ekstraksi Fitur TF-IDF

Langkah ini dilakukan untuk mengekstraksi fitur deskriptif dari data kategori game menggunakan teknik TF-IDF (*Term Frequency–Inverse Document Frequency*), yang bertujuan untuk merepresentasikan setiap game dalam bentuk vektor numerik berdasarkan kemunculan kata-kata unik pada kategorinya. Proses ini mencakup inisialisasi `TfidfVectorizer`, pelatihan model terhadap kolom `category`, serta transformasi data menjadi *sparse matrix*.
"""

# Inisialisasi TfidfVectorizer
tfidf  = TfidfVectorizer()

# Melakukan perhitungan idf pada data category
tfidf .fit(data_game['category'])

# Mapping array dari fitur index integer ke fitur nama
tfidf .get_feature_names_out()

# Melakukan fit lalu ditransformasikan ke bentuk matrix
tfidf_matrix = tfidf .fit_transform(data_game['category'])

# Melihat ukuran matrix tfidf
tfidf_matrix.shape

# Mengubah vektor tf-idf dalam bentuk matriks dengan fungsi todense()
tfidf_matrix.todense()

# Membuat dataframe untuk melihat tf-idf matrix
# Kolom diisi dengan jenis category
# Baris diisi dengan nama game

pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tf.get_feature_names_out(),
    index=data_game.gameName
).sample(min(22, tfidf_matrix.shape[1]), axis=1).sample(min(10, tfidf_matrix.shape[0]), axis=0) # Changed here

"""Vektor TF-IDF ini memungkinkan sistem untuk menghitung kemiripan antar game berdasarkan konten deskriptifnya, yang menjadi inti dari pendekatan *content-based filtering*. Dapat dilihat telah terbentuknya representasi fitur yang menunjukkan seberapa penting suatu kategori dalam membedakan satu game dengan game lainnya.

### Collaborative Filtering Preparation

#### Encoded Data

Langkah ini dilakukan untuk menyiapkan data review dalam format numerik yang dibutuhkan oleh algoritma *collaborative filtering*, khususnya dalam membangun model berbasis pembelajaran mesin seperti neural network. Proses ini mencakup pembulatan dan konversi nilai `rating` menjadi tipe `int`, serta melakukan encoding terhadap `username` dan `gameId` ke dalam representasi numerik agar dapat diproses oleh model.
"""

data_review['rating'] = data_review['rating'].round().astype(int)
data_review.sample(5)

# Mengubah userID menjadi list tanpa nilai yang sama
user_ids = data_review['username'].unique().tolist()
print('list userID: ', user_ids)

# Melakukan encoding userID
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded userID : ', user_to_user_encoded)

# Melakukan proses encoding angka ke ke userID
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke userID: ', user_encoded_to_user)

# Mengubah gameId menjadi list tanpa nilai yang sama
game_ids = data_review['gameId'].unique().tolist()

# Melakukan proses encoding gameId
game_to_game_encoded = {x: i for i, x in enumerate(game_ids)}

# Melakukan proses encoding angka ke gameId
game_encoded_to_game = {i: x for i, x in enumerate(game_ids)}

# Mapping username ke dataframe user
data_review['user'] = data_review['username'].map(user_to_user_encoded)

# Mapping gameId ke dataframe game
data_review['game'] = data_review['gameId'].map(game_to_game_encoded)

# Mendapatkan jumlah user
num_users = len(user_to_user_encoded)
print(num_users)

# Mendapatkan jumlah game
num_game = len(game_encoded_to_game)
print(num_game)

# Mengubah rating menjadi nilai float
data_review['rating'] = data_review['rating'].values.astype(np.float32)

# Nilai minimum rating
min_rating = min(data_review['rating'])

# Nilai maksimal rating
max_rating = max(data_review['rating'])

print('Number of User: {}, Number of Game: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_game, min_rating, max_rating
))

"""Hasil dari encoding disimpan dalam kolom `user` dan `game`, sementara informasi jumlah pengguna dan game juga dihitung untuk menentukan dimensi input pada tahap pelatihan model. Dataset hasil encoding terdiri dari 312.420 pengguna unik dan 18.887 game unik, dengan rentang rating dari 0.0 hingga 10.0. Skala rating yang ada cukup luas memungkinkan model untuk menangkap variasi preferensi pengguna secara lebih detail, namun juga memerlukan perhatian dalam normalisasi. Selain itu, jumlah pengguna dan game yang besar menunjukkan bahwa dataset cukup kaya dan potensial untuk membangun model rekomendasi yang akurat, tetapi juga membutuhkan optimisasi performa saat training.

#### Label Normalization

Langkah ini dilakukan untuk menyiapkan data input (`x`) dan target output (`y`) yang akan digunakan dalam pelatihan . Dataset diacak terlebih dahulu agar distribusi data menjadi acak dan tidak bias urutan. Selanjutnya, pasangan `user` dan `game` dikombinasikan sebagai fitur input, sementara nilai `rating` dinormalisasi ke rentang 0 hingga 1 menggunakan teknik *min-max scaling*.
"""

# Mengacak dataset
data_review = data_review.sample(frac=1, random_state=42)
data_review

# Membuat variabel x untuk mencocokkan data user dan game menjadi satu value
x = data_review[['user', 'game']].values

# Membuat variabel y untuk membuat rating dari hasil
y = data_review['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

"""Normalisasi ini penting untuk mempercepat konvergensi model dan menjaga kestabilan pelatihan, terutama ketika menggunakan model berbasis neural network. Hasilnya telah terbentuk pasangan data input-output dalam skala yang seragam, siap digunakan untuk proses training model.

#### Data Splitting

Langkah ini dilakukan untuk membagi data menjadi **data latih (80%)** dan **data validasi (20%)**, yang merupakan praktik umum dalam pelatihan model machine learning untuk mengevaluasi performa model terhadap data yang belum pernah dilihat sebelumnya.
"""

# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * data_review.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

"""Data input (`x`) yang berisi pasangan `(user, game)` dan target (`y`) yang berisi *rating* ter-normalisasi dibagi berdasarkan indeks, tanpa mengubah urutan acak yang sudah diterapkan sebelumnya.

## Modeling and Result

Proses ini dibagi menjadi dua, yang pertama model development untuk content based filtering dan satunya lagi untu collaborative filtering, diakhir tiap tahap development juga dilakukan inference yang akan menghasilkan result rekomendasi game yang didapat.

### Model Development Content Based Filtering

#### Menghitung nilai cosine_similarity

Langkah ini dilakukan untuk mengembangkan sistem rekomendasi berbasis konten (*content-based filtering*) dengan menghitung kemiripan antar game menggunakan metrik **cosine similarity** terhadap representasi TF-IDF dari kategori game.
"""

# Menghitung cosine similarity pada matrix tf-idf
cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

# Membuat dataframe dari variabel cosine_sim dengan baris dan kolom berupa nama game
cosine_sim_df = pd.DataFrame(cosine_sim, index=data_game['gameName'], columns=data_game['gameName'])
print('Shape:', cosine_sim_df.shape)

# Melihat similarity matrix pada setiap game
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""Hasilnya adalah sebuah matriks kemiripan dua dimensi yang menunjukkan seberapa mirip satu game dengan game lainnya berdasarkan deskripsi kategorinya.Sehingga telah terbentuk peta hubungan antar game berdasarkan konten mereka, yang menjadi dasar dalam merekomendasikan game-game serupa kepada pengguna berdasarkan preferensi sebelumnya.

#### Membuat fungsi rekomendasi

Langkah ini dilakukan untuk membangun **fungsi rekomendasi berbasis konten** yang mengembalikan daftar game serupa berdasarkan kemiripan deskriptif (kategori) menggunakan nilai *cosine similarity*. Fungsi `game_recommendations` mencari `k` game dengan nilai kemiripan tertinggi terhadap game yang diberikan (dalam hal ini `"Hannibal & Hamilcar"` yang memiliki kategori ['Ancient', 'Political', 'Wargame']) sambil mengecualikan game itu sendiri dari hasil.
"""

def game_recommendations(nama_game, similarity_data=cosine_sim_df, items=data_game[['gameName', 'category']], k=5):

    # Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan
    # Dataframe diubah menjadi numpy
    # Range(start, stop, step)
    index = similarity_data.loc[:,nama_game].to_numpy().argpartition(
        range(-1, -k, -1))

    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index[-1:-(k+2):-1]]

    # Drop nama_game agar nama game yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(nama_game, errors='ignore')

    return pd.DataFrame(closest).merge(items).head(k)

data_game[data_game.gameName.eq('Hannibal & Hamilcar')]

# Mendapatkan rekomendasi game yang mirip
print("Top 10 Rekomendasi Game:")
game_recommendations('Hannibal & Hamilcar')

"""Hasilnya adalah sistem mampu mengembalikan rekomendasi game yang sangat relevan secara tematis—seperti game ['Ancient', 'Political', 'Wargame'] lainnya—yang menunjukkan bahwa pendekatan TF-IDF + *cosine similarity* efektif dalam menangkap kesamaan semantik antar game.

### Model Development Collaborative Filtering

Langkah ini dilakukan untuk mengembangkan model rekomendasi berbasis collaborative filtering menggunakan neural network embedding yang mempelajari representasi laten dari pengguna dan game berdasarkan data interaksi mereka (rating).
"""

class RecommenderNet(tf.keras.Model):

  # Insialisasi fungsi
  def __init__(self, num_users, num_game, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_game = num_game
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.game_embedding = layers.Embedding( # layer embeddings game
        num_game,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.game_bias = layers.Embedding(num_game, 1) # layer embedding game bias

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    game_vector = self.game_embedding(inputs[:, 1]) # memanggil layer embedding 3
    game_bias = self.game_bias(inputs[:, 1]) # memanggil layer embedding 4

    dot_user_game = tf.tensordot(user_vector, game_vector, 2)

    x = dot_user_game + user_bias + game_bias

    return tf.nn.sigmoid(x) # activation sigmoid

"""Model `RecommenderNet` menggunakan layer embedding untuk mengkonversi ID pengguna dan game ke dalam vektor berdimensi rendah, dan menghitung prediksi rating melalui operasi *dot product* ditambah bias, dengan aktivasi sigmoid untuk mengoutput nilai dalam rentang 0–1.

"""

model = RecommenderNet(num_users, num_game, 50) # inisialisasi model

reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,         # turunkan learning rate jadi setengah
    patience=3,         # setelah 3 epoch tanpa perbaikan
    verbose=1,
    min_lr=1e-6
)

# model compile
model.compile(
    loss = tf.keras.losses.MeanSquaredError(),
    optimizer=keras.optimizers.Adam(learning_rate=0.001),
    metrics=[
        tf.keras.metrics.RootMeanSquaredError(),
        tf.keras.metrics.MeanAbsoluteError(),
        tf.keras.metrics.MeanSquaredError()
    ]
)

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 512,
    epochs = 50,
    validation_data = (x_val, y_val),
    callbacks = [reduce_lr]
)

"""Model dilatih menggunakan data training dan divalidasi dengan data validasi, dengan metrik evaluasi RMSE, MAE, dan MSE untuk memantau performa. pada epoch 50 atau terakhir didapat loss training sebesar 0.1028 dan loss validasi sebesar 0.1085, yang mencerminkan perbedaan kecil antara performa pada data latih dan data validasi. Metrik Root Mean Squared Error (RMSE) pada data validasi sebesar 0.2601, dan Mean Absolute Error (MAE) sebesar 0.2179, menunjukkan bahwa model mampu memprediksi rating dengan rata-rata kesalahan kurang dari satu skala rating (0–1 karena dinormalisasi).

"""

game_df = data_game
df = data_review

# Mengambil sample user
user_id = df.username.sample(1).iloc[0]
game_rated_by_user = df[df.username == user_id]

# Operator bitwise (~), bisa diketahui di sini https://docs.python.org/3/reference/expressions.html
game_not_rated = game_df[~game_df['gameId'].isin(game_rated_by_user.gameId.values)]['gameId']
game_not_rated = list(
    set(game_not_rated)
    .intersection(set(game_to_game_encoded.keys()))
)

game_not_rated = [[game_to_game_encoded.get(x)] for x in game_not_rated]
user_encoder = user_to_user_encoded.get(user_id)
user_game_array = np.hstack(
    ([[user_encoder]] * len(game_not_rated), game_not_rated)
)

ratings = model.predict(user_game_array).flatten()

top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_game_ids = [
    game_encoded_to_game.get(game_not_rated[x][0]) for x in top_ratings_indices
]

print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('Game with high ratings from user')
print('----' * 8)

top_game_user = (
    game_rated_by_user.sort_values(
        by = 'rating',
        ascending=False
    )
    .head(5)
    .gameId.values
)

game_df_rows = game_df[game_df['gameId'].isin(top_game_user)]
for row in game_df_rows.itertuples():
    print(row.gameName, ':', row.category)

print('----' * 8)
print('Top 10 game recommendation')
print('----' * 8)

recommended_game = game_df[game_df['gameId'].isin(recommended_game_ids)]
for row in recommended_game.itertuples():
    print(row.gameName, ':', row.category)

"""User josephcasey memiliki preferensi kuat terhadap game dengan tema fantasi, strategi, dan eksplorasi, Model berhasil merekomendasikan game yang konsisten dengan pola yang sama. Model tidak hanya merekomendasikan game yang identik, tapi juga menggeneralisasi preferensi user. Rekomendasi seperti Brass: Birmingham dan Caverna: The Cave Farmers menunjukkan bahwa model juga mendorong user untuk mencoba genre baru yang masih relevan, yaitu economic dan resource management, yang mungkin belum terlihat secara eksplisit di game yang sebelumnya disukai user.

## Evaluation

### Content Based Filtering Evaluation
"""

def evaluate_game_recommendations(data_game, cosine_sim_df, k=5, sample_size=200):

    precision_list = []
    recall_list = []
    f1_list = []
    map_list = []
    intra_sim_list = []
    coverage_set = set()

    k = min(k, len(data_game) - 1)
    sample_size = min(sample_size, len(data_game))
    sample_games = data_game.sample(n=sample_size, random_state=42)

    for _, game in sample_games.iterrows():
        try:
            recs = game_recommendations(
                game['gameName'],
                similarity_data=cosine_sim_df,
                items=data_game[['gameName', 'category']],
                k=k
            )

            if len(recs) > 0:
                rec_names = recs['gameName'].tolist()
                relevant_items = data_game[data_game['category'] == game['category']]['gameName'].tolist()
                relevant_items = [item for item in relevant_items if item != game['gameName']]

                # Precision, Recall, F1
                relevant_recs = [item for item in rec_names if item in relevant_items]
                precision = len(relevant_recs) / k
                recall = len(relevant_recs) / len(relevant_items) if len(relevant_items) > 0 else 0
                f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

                # MAP@k
                hits = 0
                sum_precisions = 0
                for i, item in enumerate(rec_names):
                    if item in relevant_items:
                        hits += 1
                        sum_precisions += hits / (i + 1)
                avg_precision = sum_precisions / min(len(relevant_items), k) if relevant_items else 0

                # Intra-list similarity
                sim_values = []
                for i, j in combinations(rec_names, 2):
                    if i in cosine_sim_df.columns and j in cosine_sim_df.index:
                        sim_values.append(cosine_sim_df.loc[j, i])
                intra_sim = np.mean(sim_values) if sim_values else 0

                precision_list.append(precision)
                recall_list.append(recall)
                f1_list.append(f1)
                map_list.append(avg_precision)
                intra_sim_list.append(intra_sim)
                coverage_set.update(rec_names)

        except Exception:
            continue

    summary_dict = {
        'precision': np.mean(precision_list) if precision_list else 0,
        'recall': np.mean(recall_list) if recall_list else 0,
        'f1_score': np.mean(f1_list) if f1_list else 0,
        'MAP@k': np.mean(map_list) if map_list else 0,
        'intra_list_similarity': np.mean(intra_sim_list) if intra_sim_list else 0,
        'coverage': len(coverage_set) / len(data_game),
        'samples_evaluated': len(precision_list)
    }

    # Format hasil bulatkan 3 desimal dan samples_evaluated jadi integer
    summary_df = pd.DataFrame([
        {'metric': k, 'hasil': f"{v:.3f}" if isinstance(v, float) else str(int(v))}
        for k, v in summary_dict.items()
    ])

    return summary_df

hasil_cb = evaluate_game_recommendations(data_game, cosine_sim_df)
hasil_cb

"""**Precision tinggi**: Model merekomendasikan game yang sangat relevan secara konten.
**MAP\@k tinggi**: Rekomendasi tidak hanya tepat, tapi juga **terurut dengan baik**.
**Cocok untuk personalisasi awal**, terutama untuk user baru (cold start).
**Recall rendah** → Banyak game relevan yang tidak muncul.
**Intra-list similarity tinggi** → Rekomendasi **terlalu mirip**, menyebabkan kejenuhan user.
**Coverage sangat rendah** → Hanya sebagian kecil game yang pernah muncul.

### Collaborative Filtering Evaluation
"""

fig, axs = plt.subplots(2, 2, figsize=(14, 10))

# 1. Loss
axs[0, 0].plot(history.history['loss'], label='Train Loss')
axs[0, 0].plot(history.history['val_loss'], label='Validation Loss')
axs[0, 0].set_title('Loss over Epochs')
axs[0, 0].set_xlabel('Epoch')
axs[0, 0].set_ylabel('Loss')
axs[0, 0].legend()
axs[0, 0].grid(True)

# 2. RMSE
axs[0, 1].plot(history.history['root_mean_squared_error'], label='Train RMSE')
axs[0, 1].plot(history.history['val_root_mean_squared_error'], label='Validation RMSE')
axs[0, 1].set_title('RMSE over Epochs')
axs[0, 1].set_xlabel('Epoch')
axs[0, 1].set_ylabel('RMSE')
axs[0, 1].legend()
axs[0, 1].grid(True)

# 3. MAE
axs[1, 0].plot(history.history['mean_absolute_error'], label='Train MAE')
axs[1, 0].plot(history.history['val_mean_absolute_error'], label='Validation MAE')
axs[1, 0].set_title('MAE over Epochs')
axs[1, 0].set_xlabel('Epoch')
axs[1, 0].set_ylabel('MAE')
axs[1, 0].legend()
axs[1, 0].grid(True)

# 4. MSE
axs[1, 1].plot(history.history['mean_squared_error'], label='Train MSE')
axs[1, 1].plot(history.history['val_mean_squared_error'], label='Validation MSE')
axs[1, 1].set_title('MSE over Epochs')
axs[1, 1].set_xlabel('Epoch')
axs[1, 1].set_ylabel('MSE')
axs[1, 1].legend()
axs[1, 1].grid(True)

plt.tight_layout()
plt.show()

"""**1. Loss over Epochs**

* **Train dan validation loss** menunjukkan tren menurun yang stabil setelah puncaknya sekitar epoch ke-3 hingga ke-5.
* Setelah itu, **tidak ada overfitting yang signifikan**, karena *validation loss* mendatar dan cukup dekat dengan *train loss*.
* Ini indikasi bahwa model **belajar dengan baik dan generalisasi cukup bagus**.

---

**2. RMSE (Root Mean Square Error)**

* RMSE untuk train dan validation juga **menurun konsisten**, walau lebih lambat dari loss.
* Gap antara train dan validation RMSE relatif kecil setelah epoch ke-20.
* **RMSE mendatar di bawah 0.26**, artinya prediksi rating cukup akurat.

---

**3. MAE (Mean Absolute Error)**

* Sama seperti RMSE, MAE menunjukkan **penurunan awal yang tajam**, lalu stabil.
* Validation MAE sekitar **0.22**, cukup bagus.
* MAE lebih stabil dibanding RMSE, menunjukkan **jumlah prediksi error besar tidak terlalu banyak**.

---

**4. MSE (Mean Squared Error)**

* MSE lebih sensitif terhadap outlier dibanding MAE, tapi di sini **perilakunya mirip dan stabil**.
* Nilainya stabil di bawah **0.07** untuk validation, menunjukkan error kuadrat kecil.

---
"""